\documentclass[11pt]{article}
\usepackage{times}

\title{CMSC621 Project 1 Report}
\author{Patrick Trinkle \& Mike Corbin\\
Dept. of Computer Science and Electrical Engineering,\\
University of Maryland Baltimore County,\\
Baltimore, MD, 21250\\
\texttt{(tri1|corbin2)@umbc.edu}}
\date{October 2009}

\begin{document}
\maketitle

\section{Introduction}
The assignment was to build a distributed solution to the problem of calculating information about a large file F, comprised of double precision floating point values.  ProjectOne will quickly determine the maximum, minimum, average and median values.

\section{Background}
A distributed system is a processing system where the computation is spread out over a series of systems.  These systems can be controlled in many ways, possibly via a primary tasking node or alternatively in a peer-to-peer tasking scenario.  With the central tasking model you can easily imagine a large task being broken into independent pieces and these tasks being assigned to a plethora of machines all over the network.  Processing redundancy as well as good book-keeping can allow this type of processing to easily restart on a node if it goes down.  If a node is no longer communicating properly (possibly because it is dead) the tasking assigned to it can be shifted to another node.  Therefore the processing is still being handled in a distributed way that is semi-fault tolerant.  As long as the cost of sending the data to be processed is less then the time to process it, this is a good solution to a large problem.  Because of this cost in data passing, not all problems that can be parallelized truly benefit from such systems.  However, for the most part--processing that can be parallelized should be.  To aid in taking a problem and solving it in a distributed way there are infrastructures in place that a developer can utilize.  Hadoop is an implementation of a distributed platform, for which a developer can write code that will work in a distributed way with most of the details hidden.  Using a pre-existing platform can save a lot of time and money in developing distributed applications.

\subsection{Hadoop}
Our system is implemented using Hadoop.  To manage the input and output files our system relies on  the Hadoop Distributed File System (HDFS).  The HDFS will automatically replicate files across nodes and manage file coherency.  The program interfaces with Hadoop's implementation of MapReduce.

\subsection{MapReduce}
MapReduce is an algorithm by which a problem is broken up into two primary steps: the map step; and the reduce step.  During the map step there are some actions on the input that produce key value pairs.  During the reduce step each reducer will handle strictly one key and the list of data associated with that key.  The reduce step may very well produce input for another iteration of the algorithm.  The reduce step also outputs one file per key in the system.  If the input was all instances of a word then the reduce step could output the number of instances of each of the words (keys).  Hadoop provides an abstraction/interface for building programs that utilize MapReduce.  Part of what is provided is file handling.  Hadoop will split a file and send it out across the available nodes during the mapping step automatically.  A developer can for the most part modify an interface to this file splitting to leverage how much is sent at a time.  The keys value pairs from the mapping step are automatically captured by the system and stored in an intermediate storage location; and sorted by key.  Each key is mapped to a reducer process that can be on any of the nodes.  If the map step returns many different keys the processing is spread out over equally many reducers.

\section{Implementation}
We first implemented a our own import format class that can read in multiple lines at a time.  We did this because are input file contains one number on each line, and if we only read one line at a time the first mapping phase could do very little work.  By reading in multiple lines can do more work in the inital mapping phase which lets us find each answer in one map reduce cycle.

Our average and max, min and such are calculated as such.  Our ProjectOne calcaluates the minimum and maximum values, simultaneously walking through the data received.  The average is calculated separately strictly because of an implementation detail wherein the output collect for mappers can only take a set type from a mapper.  Therefore if the mapper returns a text key and a text value it must always return such.  To save on value conversions our maximum/minimum mapper return a text key with a double value.  The average mapper needs to return two pieces of information that must be kept together and as such the information is dumped as a text.  (Consider changing maxmin to return a text value and such and see if it's faster overall versus running maxmin, avg, then med (it'd just be maxminavg, med)).  If we could save sufficient time in processing it might outweigh the processing of value conversion.  Especially because for the x values we receive we only have to convert 1/100; assuming they get 100 lines each.  Might be worth examining.

Our median calculator estimates the median by examining the median of the medians.  Insert some details here regarding the worst cases (30\%/70\%).  Also, might be worth involving details about our medians of medians in groups of five.

\end{document}
